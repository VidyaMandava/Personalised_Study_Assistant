{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Develop personalized Study Assistant\n",
        "  - Reads a PDF from Google Drive and extracts the study material.\n",
        "  - Generates a summary for a given topic.\n",
        "  - Runs an adaptive multiplechoice question (MCQ) assessment (10 questions total) where the question complexity starts at medium, increases when the student is correct, and decreases when the student is wrong.\n",
        "  - At the end, it prints the final score.\n"
      ],
      "metadata": {
        "id": "O-Dc3sSnpmer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.3.11\n",
        "!pip install langchain-community==0.3.11\n",
        "!pip install PyPDF2\n",
        "!pip install langchain-google-genai"
      ],
      "metadata": {
        "id": "4XeNTC6WsFBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "GOOGLE_API_KEY = getpass('Please enter your Gemini Token here: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGaG5sAGsfeE",
        "outputId": "960339e0-9144-4585-c92c-57ed7f7f83fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your Gemini Token here: 路路路路路路路路路路\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Environment variable\n",
        "import os\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "BjSBhVDbssSQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)"
      ],
      "metadata": {
        "id": "ktMND4wZ8SYL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PDF Parsing**"
      ],
      "metadata": {
        "id": "BeC6Nh6r9XN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "pdf_path = '/content/drive/MyDrive/Study_material.pdf'\n",
        "study_material = extract_text_from_pdf(pdf_path)\n",
        "display(Markdown(f\"**Extracted study material length:** {len(study_material)}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "HCR7hlGN9Ng1",
        "outputId": "4c71e81c-ce22-4601-bd24-6c8f1417025a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Extracted study material length:** 116669"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary Chain"
      ],
      "metadata": {
        "id": "0R3wA1_t9oDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_prompt = \"\"\"\n",
        "Act as a study assistant by providing a concise summary of the topic provided below, enclosed within triple backticks.\n",
        "Utilize only the learning material provided below, enclosed within triple backticks, to generate the summary.\n",
        "Ensure that your response is user-friendly and adaptable to various educational subjects.\n",
        "Focus solely on the information from the learning material without introducing external content.\n",
        "\n",
        "Format:\n",
        "# {topic}\n",
        "summary...\n",
        "\n",
        "Learning Material:\n",
        "```{study_material}```\n",
        "Topic:\n",
        "```{topic}```\n",
        "\"\"\"\n",
        "summary_template = ChatPromptTemplate.from_template(summary_prompt)\n",
        "partial_summary_prompt = summary_template.partial(study_material=study_material)\n",
        "\n",
        "summary_chain = (\n",
        "    partial_summary_prompt\n",
        "    | gemini\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "def generate_summary(topic):\n",
        "    result = summary_chain.invoke({\"topic\": topic})\n",
        "    return result"
      ],
      "metadata": {
        "id": "xnuniPkO9aUZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MCQ model"
      ],
      "metadata": {
        "id": "ff1kSdKT-AxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "mcq_prompt_hidden_answer = \"\"\"\n",
        "You are an intelligent tutor. Based solely on the study material provided, generate a {difficulty} multiple-choice question on the topic: {topic}.\n",
        "Provide four answer options labeled A, B, C, and D.\n",
        "**Do not** display the correct answer in the question text.\n",
        "Instead, append a separate line at the end in the format:\n",
        "SOLUTION: <Correct Option>\n",
        "\n",
        "Example Format:\n",
        "Q: ...\n",
        "A) ...\n",
        "B) ...\n",
        "C) ...\n",
        "D) ...\n",
        "SOLUTION: B\n",
        "\n",
        "Study Material:\n",
        "```{study_material}```\n",
        "\"\"\"\n",
        "\n",
        "mcq_template_hidden_answer = ChatPromptTemplate.from_template(mcq_prompt_hidden_answer)\n",
        "partial_mcq_prompt_hidden = mcq_template_hidden_answer.partial(study_material=study_material)\n",
        "\n",
        "mcq_chain_hidden = (\n",
        "    partial_mcq_prompt_hidden\n",
        "    | gemini\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2. Generate and Parse the MCQ Without Displaying the Correct Answer\n",
        "# -------------------------------------------------------------------\n",
        "def generate_mcq_hidden(topic, difficulty):\n",
        "    \"\"\"Generates an MCQ with the correct answer hidden in a separate line.\"\"\"\n",
        "    result = mcq_chain_hidden.invoke({\"topic\": topic, \"difficulty\": difficulty})\n",
        "    return result\n",
        "\n",
        "def parse_mcq_hidden(mcq_text):\n",
        "    \"\"\"\n",
        "    We look for the 'SOLUTION: X' line to extract the correct answer\n",
        "    but do not display it to the user.\n",
        "    \"\"\"\n",
        "\n",
        "    # Split on 'SOLUTION:'\n",
        "    parts = mcq_text.split(\"SOLUTION:\")\n",
        "    if len(parts) < 2:\n",
        "       return mcq_text, None\n",
        "\n",
        "    question_text = parts[0].strip()\n",
        "    correct_line = parts[1].strip()\n",
        "\n",
        "    match = re.search(r\"([A-D])\", correct_line, re.IGNORECASE)\n",
        "    correct_answer = match.group(1).upper() if match else None\n",
        "\n",
        "    return question_text, correct_answer\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3. Ask MCQ Function\n",
        "# -------------------------------------------------------------------\n",
        "def ask_mcq_hidden(topic):\n",
        "    difficulty_levels = ['easy', 'medium', 'hard']\n",
        "    current_difficulty = 1  # start at medium\n",
        "    score = 0\n",
        "\n",
        "    mcq_results = []\n",
        "\n",
        "    for i in range(10):\n",
        "        question_info = f\"\\n**Question {i+1} (Difficulty: {difficulty_levels[current_difficulty]})**\"\n",
        "\n",
        "        mcq_text = generate_mcq_hidden(topic, difficulty_levels[current_difficulty])\n",
        "        question_text, correct_answer = parse_mcq_hidden(mcq_text)\n",
        "\n",
        "        # store the question + answer in Markdown format\n",
        "        question_block = f\"{question_info}\\n\\n{question_text}\"\n",
        "\n",
        "        if not correct_answer:\n",
        "            # If the correct answer can't be parsed, we skip\n",
        "            error_msg = f\"{question_block}\\n\\n**Error:** Could not parse the correct answer. Skipping this question.\"\n",
        "            mcq_results.append(error_msg)\n",
        "            continue\n",
        "\n",
        "        # Print question to console (for the interactive input)\n",
        "        print(question_block)\n",
        "        student_answer = input(\"Your answer (A/B/C/D): \").strip().upper()\n",
        "\n",
        "        if student_answer == correct_answer:\n",
        "            feedback = f\"**Correct!**\"\n",
        "            score += 1\n",
        "            if current_difficulty < 2:\n",
        "                current_difficulty += 1\n",
        "        else:\n",
        "            feedback = f\"**Incorrect!** The correct answer was **{correct_answer}**.\"\n",
        "            if current_difficulty > 0:\n",
        "                current_difficulty -= 1\n",
        "\n",
        "        # Combine question and feedback in one Markdown message\n",
        "        mcq_results.append(f\"{question_block}\\n\\n{feedback}\")\n",
        "\n",
        "    # Return both the collected messages and the final score\n",
        "    return mcq_results, score\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4. Integrate into Existing Pipeline\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "def study_assessment_pipeline(pdf_path, topic):\n",
        "    display(Markdown(\"##  Hello there! Am your Study Assistant!\"))\n",
        "\n",
        "    # display(Markdown(\"###  Generating Summary on (topic)\"))\n",
        "    summary = generate_summary(topic)\n",
        "    display(Markdown(f\"###  **Summary on {topic}**\\n{summary}\"))\n",
        "\n",
        "    display(Markdown(\"###  Starting MCQ Assessment...\"))\n",
        "    mcq_results, final_score = ask_mcq_hidden(topic)\n",
        "\n",
        "    display(Markdown(f\"###  **Final Score:** {final_score}/10\"))\n",
        "\n",
        "# Now run your pipeline as usual:\n",
        "study_assessment_pipeline(pdf_path, \"Question Answering\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nIi3ub68fe7t",
        "outputId": "32ab34bf-0401-4a2a-d7fa-11dccd0697f4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "##  Hello there! Am your Study Assistant!"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "###  **Summary on Question Answering**\n# Question Answering\n\nNatural Language Processing (NLP) in education uses techniques like question answering (QA) to help students and teachers.  Two main subcategories are Textbook Question Answering (TQA) and Math Word Problem (MWP) solving.  TQA involves understanding multimodal information (text, images, diagrams) from textbooks to answer questions, while MWP solving translates narrative descriptions into mathematical expressions.  Challenges include comprehending complex questions and contexts, and bridging the gap between natural language and abstract representations.  Solutions involve deep learning methods (RNNs, CNNs), attention mechanisms, and pre-training with domain-specific knowledge.  Large Language Models (LLMs) are increasingly used, often with strategies like Chain-of-Thought prompting to improve reasoning and zero/few-shot capabilities.  Datasets like SCIENCE QA and GSM8K are used for evaluation."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "###  Starting MCQ Assessment..."
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "**Question 1 (Difficulty: medium)**\n",
            "\n",
            "Q:  Based on the provided study material, which of the following is NOT identified as a primary NLP application within the education domain?\n",
            "\n",
            "A) Question Construction\n",
            "B) Automated Assessment\n",
            "C) Sentiment Analysis\n",
            "D) Error Correction\n",
            "Your answer (A/B/C/D): a\n",
            "\n",
            "**Question 2 (Difficulty: easy)**\n",
            "\n",
            "Q: Which of the following is NOT a sub-category of Question Answering in the education domain, as defined in the provided text?\n",
            "\n",
            "A) Textbook Question Answering\n",
            "B) Math Word Problem Solving\n",
            "C) Question Generation\n",
            "D) Medical Question Answering\n",
            "Your answer (A/B/C/D): c\n",
            "\n",
            "**Question 3 (Difficulty: medium)**\n",
            "\n",
            "Q:  Based on the provided study material, which of the following is NOT identified as a primary NLP application within the education domain?\n",
            "\n",
            "A) Question Construction\n",
            "B) Automated Assessment\n",
            "C) Sentiment Analysis\n",
            "D) Error Correction\n",
            "Your answer (A/B/C/D): d\n",
            "\n",
            "**Question 4 (Difficulty: easy)**\n",
            "\n",
            "Q: Which of the following is NOT a sub-category of Question Answering in the education domain, as defined in the provided text?\n",
            "\n",
            "A) Textbook Question Answering\n",
            "B) Math Word Problem Solving\n",
            "C) Question Generation\n",
            "D) Medical Question Answering\n",
            "Your answer (A/B/C/D): a\n",
            "\n",
            "**Question 5 (Difficulty: easy)**\n",
            "\n",
            "Q: Which of the following is NOT a sub-category of Question Answering in the education domain, as defined in the provided text?\n",
            "\n",
            "A) Textbook Question Answering\n",
            "B) Math Word Problem Solving\n",
            "C) Question Generation\n",
            "D) Medical Question Answering\n",
            "Your answer (A/B/C/D): c\n",
            "\n",
            "**Question 6 (Difficulty: medium)**\n",
            "\n",
            "Q:  Based on the provided study material, which of the following is NOT identified as a primary NLP application within the education domain?\n",
            "\n",
            "A) Question Construction\n",
            "B) Automated Assessment\n",
            "C) Sentiment Analysis\n",
            "D) Error Correction\n",
            "Your answer (A/B/C/D): b\n",
            "\n",
            "**Question 7 (Difficulty: easy)**\n",
            "\n",
            "Q: Which of the following is NOT a sub-category of Question Answering in the education domain, as defined in the provided text?\n",
            "\n",
            "A) Textbook Question Answering\n",
            "B) Math Word Problem Solving\n",
            "C) Question Generation\n",
            "D) Medical Question Answering\n",
            "Your answer (A/B/C/D): a\n",
            "\n",
            "**Question 8 (Difficulty: easy)**\n",
            "\n",
            "Q: Which of the following is NOT a sub-category of Question Answering in the education domain, as defined in the provided text?\n",
            "\n",
            "A) Textbook Question Answering\n",
            "B) Math Word Problem Solving\n",
            "C) Question Generation\n",
            "D) Medical Question Answering\n",
            "Your answer (A/B/C/D): d\n",
            "\n",
            "**Question 9 (Difficulty: easy)**\n",
            "\n",
            "Q: Which of the following is NOT a sub-category of Question Answering in the education domain, as defined in the provided text?\n",
            "\n",
            "A) Textbook Question Answering\n",
            "B) Math Word Problem Solving\n",
            "C) Question Generation\n",
            "D) Medical Question Answering\n",
            "Your answer (A/B/C/D): c\n",
            "\n",
            "**Question 10 (Difficulty: medium)**\n",
            "\n",
            "Q:  Based on the provided study material, which of the following is NOT identified as a primary NLP application within the education domain?\n",
            "\n",
            "A) Question Construction\n",
            "B) Automated Assessment\n",
            "C) Sentiment Analysis\n",
            "D) Error Correction\n",
            "Your answer (A/B/C/D): b\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "###  **Final Score:** 3/10"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fCR_r90NpWa9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}